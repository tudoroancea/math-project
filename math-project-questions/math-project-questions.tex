\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} % pour le format de sortie
\usepackage[a4paper]{geometry}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % pour les accents
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{amssymb,amsmath,amsthm, amsfonts} % math libraries (amsthm : unumbered theorems)
\usepackage{mathtools} % for psmallmatrix
\usepackage{fancyhdr,multicol,accents, bbm,subcaption,caption,float,verbatim}
\usepackage[all]{xy} % for diagrams with arrows
\usepackage{tikz-cd} % for diagrams with arrows
\usepackage{graphicx} % to manage images
\usepackage{titlesec}
\usepackage{biblatex} % for bibliography
\usepackage{hyperref} % for hyperlinks to refs or bibliography
\usepackage{url}

% Margins, font size =====================================================================================================================
%\oddsidemargin = 0.5cm \evensidemargin = 0.5cm \textwidth = 6.3in
%\oddsidemargin = 1.2cm \evensidemargin = 1.2cm \textwidth = 6.3in
%\textheight =8.6in
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}

% Sections (theorems, propositions, lemmas…) =====================================================================================================================
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{conjecture}{\bf Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\numberwithin{theorem}{section} % To display the section number in the theorem

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem*{solution}{Solution}
\newtheorem*{answer}{Answer}
\newtheorem*{claim}{Claim}

\theoremstyle{remark}
\newtheorem*{theoremno}{{\bf Theorem}}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{hint}{Hint}



% Commands =====================================================================================================================
\def\bb#1{\mathbb{#1}}
\def\cal#1{\mathcal{#1}}
\def\frak#1{\mathfrak{#1}}
\def\rm#1{\mathrm{#1}}
\def\bf#1{\mathbf{#1}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\frakA}{\mathfrak{A}}
\newcommand{\frakS}{\mathfrak{S}}
\newcommand{\esp}{\mathbb{E}}
% \P = caracs spéciaux,\S = paragraphe, \L = L barre

% Existe déjà : ker, partie Im, Re, min, max, inf, sup, log, exp, sin, sinh, cos, cosh,, tan lim, liminf, limsup
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Homeo}{Homeo}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Bij}{Bij}
\DeclareMathOperator{\Isom}{Isom}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\rang}{rang}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\spec}{spec}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\pgcd}{pgcd}
\DeclareMathOperator{\pgdc}{pgdc}

% Lois de probabilités
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Student}{Student}
\DeclareMathOperator{\Poi}{Poi}
\newcommand{\czero}{\calC^0}
\newcommand{\cone}{\calC^1}
\newcommand{\ctwo}{\calC^2}
\newcommand{\cinf}{\calC^{\infty}}
\newcommand{\bigpeter}[1]{\Big\langle#1\Big\rangle}
\newcommand{\peter}[1]{\langle#1\rangle}
\newcommand{\transp}[1]{#1^t}
\newcommand{\series}[2]{\sum_{#1}^{\infty}#2}
\newcommand{\intt}[4]{\int_{#1}^{#2}#3\mathrm{d}#4}
\newcommand{\ddt}[1]{\frac{\mathrm{d}#1}{\mathrm{dt}}}
\newcommand{\deldt}[1]{\frac{\partial#1}{\partial\mathrm{t}}}
\newcommand{\rmd}[1]{\mathrm{d}#1}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\dx}{\rmd x}
\newcommand{\dy}{\rmd y}
\newcommand{\dz}{\rmd z}
\newcommand{\dt}{\rmd t}
\newcommand{\du}{\rmd u}
\newcommand{\dv}{\rmd v}
\newcommand{\ds}{\rmd s}
\newcommand{\dxy}{\rmd xy}
\newcommand{\dyz}{\rmd yz}
\newcommand{\dyx}{\rmd yx}
\newcommand{\dzy}{\rmd zy}
\newcommand{\dzx}{\rmd zx}
\newcommand{\dxz}{\rmd xz}
\newcommand{\gtinf}[1]{\underset{#1\to\infty}{\longrightarrow}}
\newcommand{\sm}[4]{\begin{psmallmatrix}#1&#2\\#3&#4\end{psmallmatrix}}
\newcommand{\map}[4]{
	\begin{matrix}
		#1&\to&#2\\#3&\mapsto&#4
	\end{matrix}
}

\title{Questions for meeting 4 (23/03/2022)}
\author{Tudor Oancea}
\date{}

\begin{document}
\maketitle

\noindent Here are my questions in decreasing order of importance :
\begin{enumerate}
	\item 
	Basically my first and most important question is : what should I actually prove ?
	There are actually 4 different formulations in the paper on the Relaxed Logarithmic Barrier Function Based Linear MPC and I don't know which one I should use, or if I should try to generalize all of them.
	\item[] \textcolor{red}{This one we have already been talked since we firstly discussed the problem in details. We wanna to work out the one with terminal cost but no terminal constraint at the initial stage. Well, it should be also possible to build the theory if you consider the terminal constraint. Basically, you only need to assume that terminal constraint is control forward invariant in nonlinear case as the book proposed.}
	\item I still don't understand the cubic error equation $V_\infty(x)=M(x)+O(||x||^3)$ in your paper so could we see a more explicit proof tomorrow ? Also in the Riccati equation associated shouldn't it be $A^T$ instead of $A$ in the last term ?
	
	\item[] \textcolor{red}{Regarding your first question, we need firstly recall the problem considered in my previous paper, according to the assumption maded in that paper, we basically have no constraint active locally such that we have 
	\[
	\begin{aligned}
	V_N(x_0) =& \min_{u,x}\;\;\sum_{k=0}^{N-1}\ell(x_k,u_k) + M(x_N) \quad \text{s.t.}\;\;x_{k+1} = f(x_k,u_k)\\
	V_\infty(x_0) =& \min_{u,x}\;\;\sum_{k=0}^{\infty}\ell(x_k,u_k) \quad \text{s.t.}\;\;x_{k+1} = f(x_k,u_k)\\
	M(x_0) =& \min_{u,x}\;\;\sum_{k=0}^{\infty}x_k^\top Q x_k + 2 x_k^\top S u_k + u_k^\top R u_k \quad \text{s.t.}\;\;x_{k+1} = A x_k + B u_k
	\end{aligned}
	\]
	Here, one can see that $M(x_0) = x_0^\top P x_0$ follows the dynamic programming and the matrix $P$ is the solution of the algebraic Riccati equation below. In order to show the cubic error term, we first need to work out the derivative of $V_\infty(\cdot)$ at the origin. We have known already known that $V_\infty(\cdot)\geq  0$ and $V_\infty(x)=0$ holds at $x=0$. According to the assumption that $\ell $ and $f$ are twice Lipschitz-continuously differentiable locally near the origin, we have $V_\infty$ is also twice continuously differentiable (this follows the implicit function theorem if you work out the optimality condition by using the dynamic programming, i.e., 
	\begin{equation}\label{eq::optimality}
	V_\infty(x) = \min_u\;\ell(x,u)+V_\infty(f(x,u)),
	\end{equation} 
	and
	$
	\nabla V_\infty(0) =0 
	$
	as $x=0$ is the minimizer or stationary point of $V_\infty$. Therefore, we have 
	\[
     V_\infty(x) = x^\top \underbrace{\left(\frac{1}{2} \nabla^2 V_\infty(0)\right)}_{P} x + \mathbf{O}(\|x\|^3) 
	\]
    such that we need to show $\frac{1}{2}\nabla^2 V_\infty(0)=P$ (note that we have considered scaling $1/2$ in the definition of $Q$, $R$ and $S$). It is clear that if you do second orde Taylor expansion of the optimality condition~\eqref{eq::optimality}, you will get the definition of $M(\cdot)$ such that the $\frac{1}{2}\nabla^2 V_\infty(0)$ can be obtained by solving the algebraic Riccati equation, i.e., 
    \[
    M(x) = \min_u x_k^\top Q x_k + 2 x_k^\top S u_k + u_k^\top R u_k + M(Ax+Bu)
    \]
	}
	Well, I am ready to assume that $V_\infty$ is indeed $C^2$ and that $V_\infty(0)=0,~\nabla V_\infty(0)=0$ so that we can write $V_\infty(x) = x^\top \left(\frac{1}{2} \nabla^2 V_\infty(0)\right) x + \mathbf{O}(\|x\|^3) $\,.
	Also that with the implicit function theorem, if we write $u^*(x)$ the optimizer of \ref{eq::optimality} then $u^*$ is differentiable and that $\|u^*(x)\|=\mathbf{O}(\|x\|)$.
	Now I don't understand how to differentiate \ref{eq::optimality} because it's an optimization problem.
	\begin{align*}
		V_\infty(x) &= \min_u\;\ell(x,u)+V_\infty(f(x,u))\\
		&=\min_u\; x^\top Q x + 2 x^\top S u + u^\top R u +\mathbf{O}(\|x\|^3+\|u\|^3)+ V_\infty(f(x,u))\\
		&=\mathbf{O}(\|x\|^3)+\min_u\; x^\top Q x + 2 x^\top S u + u^\top R u + V_\infty(Ax+Bu+\mathbf{O}(\|x\|^2+\|u\|^2))\\
		&=\mathbf{O}(\|x\|^3)+\min_u\; x^\top Q x + 2 x^\top S u + u^\top R u + V_\infty(Ax+Bu)\\
		&\quad\quad\quad\quad\quad\quad+(Ax+Bu)^\top\left(\frac{1}{2}\nabla^2V_\infty(0)\right)\mathbf{O}(\|x\|^2+\|u\|^2)+\mathbf{O}(\|x\|^4+\|u\|^4)\\
		&\quad\quad\quad\quad\quad\quad+\mathbf{O}(\|Ax+Bu+\mathbf{O}(\|x\|^2+\|u\|^2)\|^3)\\
		&=\mathbf{O}(\|x\|^3)+\min_u\; x^\top Q x + 2 x^\top S u + u^\top R u + V_\infty(Ax+Bu)\\
	\end{align*}
	
	
	$$f(x,u)=Ax+Bu+\mathbf{O}(\|x\|^2+\|u\|^2)$$

	\item[] \textcolor{red}{Regarding your second question, it is a typo, the riccati equation should 
	\[
	P = A^\top P A +Q - (A^\top PB + S)(R+B^\top P B)^{-1}(A^\top PB + S)^\top.
	\]}
	\item From what I understood, the difference between controllability and stabilizability is :
	\begin{itemize}[label=\textbullet]
		\item controllability : any initial state $x_0$ can be steered to any other state $x_1$ in a \textbf{finite} time
		\item stabilizability : any initial state $x_0$ can be steered to any other state $x_1$ but only in an \textbf{infinite} time
	\end{itemize}
	Is this interpretation true ?
	And also should the $x_1$ in the definition of stabilizability be stable ?
	The only clear definition I found for stabilizability was the one in Exercise 1.19 in the book, but I find it kind of limiting. Are there other more general definitions ? (for more general forms of matrices $A$ and $B$)
    \item[] \textcolor{red}{My understanding from the mathematical point of view is as follows: consider a given LTI system $x^+ = A x+ Bu$, 
    \begin{quote}
        the pair of $(A,B)$ is controllable means matrix $[B\;AB\;A^2B\;...\;A^{n_x-1}B]$ is full row rank;\\
        the pair of $(A,B)$ is (asymptotically) stabilizable means there exists a matrix $K$ such that $A+BK$ has all eigenvalues' magnitude strictly smaller than 1.
    \end{quote}
    For nonlinear system, if you need strict definitions, maybe you could have a look at another book "Nonlinear Model Predictive Control" by Lars Grüne and Jürgen Pannek. But notice that we normally talk about these two definition for linear system.}
    
	\item Is there some actual linear algebra theory that justifies the existence of solutions to the Riccati equation(s) (like the one of the form $P=Q+A^TPA-A^TPB(R+B^TPB)^{-1}B^TPA$) ? the solutions is always assumed to exist but there is never a real justification for this.
    \item[] \textcolor{red}{Read the paper "The algebraic Riccati equation: conditions for the existence and uniqueness of solutions" by Harald K.Wimmer}
	\item I started looking in the implementation of the control of the predator-prey-fishing model and since it is a continuous problem we should discretize it.
	Should I do it with direct single-shooting ? multiple-shooting ? or collocation ?
	\item[] \textcolor{red}{You can use any direct methods. I would suggest to use RK4 integrator based multiple shooting.}
\end{enumerate}

For 1. I actually thought that the most natural and easy formulation was the one without terminal set and a quadratic terminal constraint (section C2), and to prove it I wanted to use the same trick as used in the book (the argument in section 2.5.5 that we had trouble with yesterday but that I think I understand know) : we want to prove that $\hat{F}(f(x,Kx))-\hat{F}(x)+\hat{l}(x,Kx)\leq 0$ at least locally around 0 using the fact that $\hat{F}(f(x,Kx))-\hat{F}(x)+\hat{l}(x,Kx)= 0,~\forall x\in\R^{n_x}$.
But here is the catch, we don't have the last equality, only an inequality (see paper) so the only hope I found is to show that $\hat{F}(f(x,Kx))\leq \hat{F}(A_Kx)$, but I don't know under which conditions this could hold.
The problem is similar with other formulations.



\end{document}