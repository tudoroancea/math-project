\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} % pour le format de sortie
\usepackage[a4paper]{geometry}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % pour les accents
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{amssymb,amsmath,amsthm, amsfonts} % math libraries (amsthm : unumbered theorems)
\usepackage{mathtools} % for psmallmatrix
\usepackage{fancyhdr,multicol,accents, bbm,subcaption,caption,float,verbatim}
\usepackage[all]{xy} % for diagrams with arrows
\usepackage{tikz-cd} % for diagrams with arrows
\usepackage{graphicx} % to manage images
\usepackage{titlesec}
\usepackage{hyperref} % for hyperlinks to refs or bibliography
\usepackage{indentfirst} % for indenting the first line of a paragraph
\usepackage{optidef}
\usepackage[backend=bibtex, style=alphabetic]{biblatex} % for bibliography
\usepackage{bookmark}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{chronosys}
\addbibresource{refs.bib}

\include{predefs}

\title{Study of Relaxed Recentered Log-Barrier function based Nonlinear Model Predictive Control}
\author{Tudor Oancea}
\date{March 2022}

\begin{document}
\maketitle

\begin{abstract}
	In this project, we investigate the use of relaxed logarithmic barrier functions in the context of nonlinear model predictive control.
	We base our work on the one of C. Feller and C. Ebenbauer in \cite{RRLB-linear-MPC} and use one of their globally stabilizing schemes with terminal costs and without terminal sets.
	We partially extend the results on the nominal asymptotic stability of the corresponding closed-loop system and the constraint satisfaction guarantees to the case of nonlinear dynamics and show that they are satisfied only in a neighborhood of the target state.
	The theoretical results are complemented by numerical illustrations based on RTI and SQP algorithms.
\end{abstract}

\section{Introduction}
\textit{Control theory} can simply be defined as the study of a generally dynamical system whose state evolution that we can influence by the means of external parameters called \textit{controls}, or \textit{inputs}.
This theory is a very important one in numerous fields of engineering such as robotics, mechanical or chemical engineering where problems such as stabilizing a chemical reaction at a certain temperature or making an autonomous car follow the road are common.

An important subfield of control theory is \textit{optimal control} (OC), which aims at finding a \textit{control law}, i.e. a way to control a system in order to attain a certain goal, by formulating and solving an optimization problem.
For example, if one wants to stabilize the state around a certain target, or reference point that is constant in time, one will want to choose a control law that will minimize the distance between the future state (that depends on the current state and the chosen control) and the target state.
This problem of \textit{stabilization} will be the one we will consider in this paper, but it is interesting to know there are other problems for which OC is applicable, such as path tracking or optimal trajectory generation.

One of the most important (family of) algorithm(s) of OC is \textit{Model Predictive Control}, or MPC.
This procedure uses the knowledge of the system \textit{dynamic model} (the equation governing its evolution) to compute a control law that also takes the future into account.
More precisely, the control law is computed for a certain interval in time called the \textit{horizon}, and the cost along this whole horizon is minized.
In some sense, the controller tries not to take decisions it will regret afterwards.
MPC has many advantages over other classical control strategies, such as the ability to handle several control parameters and to take into consideration constraints on the states and the controls (by solving a \textit{constrained} optimization problem). These are very important in practice because any real-life system has physical limitations (e.g. the maximum speed of a car, the maximum torque of a motor, etc.).
We will properly introduce the mathematical formulation of MPC and the theoretical tools used to study it in section \ref{sec:background-material}.

In section \ref{sec:RRLB-MPC} we will introduce a different formulation in which (most of) the state and control constraints are replaced with a penalty added to the cost function.
Such penalties are common in Interior Point methods and are usually log-barrier functions.
These functions have several drawbacks, such as the fact that they are only defined in the interior of the feasible set.
In this paper we will introduce a relaxed version of these functions called \textit{Relaxed Recentered Log-Barrier functions}, or \textit{RRLB functions}.
They extend regular log-barrier functions to the whole space and yield a convex optimization problem that is in general much easier to solve than the original one.
More details on the upsides of RRLB functions can be found in the Introduction of \cite{RRLB-linear-MPC}.

In the theoretical study of this new formulation that we will call \textit{RRLB MPC}, we will prove two major results :
\begin{enumerate}
	\item When using the control law computed by the RRLB MPC, the corresponding closed-loop system asymptotically stabilizes at the target state. (see subsection \ref{sec:RRLB-nominal-stability}) This property will be called \textit{nominal stabilty}, to be differentiated of \textit{real-time stability} (see subsection \ref{sec:RRLB-real-time-stability}).
	\item In this formulation, the optimal solution of the optimization problem might yield states or controls that are not feasible in the original problem. However, if we start sufficiently close to the target state, this will never happen. (see subsection \ref{sec:constraints-satisfaction-guarantees})
\end{enumerate}

Finally, in section \ref{sec:RRLB-numerical-aspects} we examine numercial aspects of the RRLB MPC.
In practice, the optimization problem is not solved exactly, and we can't necessarily guarantee the satisfaction of the previous results in practice.
However, we will discuss how using \textit{Real Time Iteration} methods (or RTI, a variant of Sequential Quadratic Programming with only one iteration), can meet the gap between computational efficiency and the accuracy of the results.
All our results will in the end be illustrated with a classical benchmark problem for MPC schemes : the Continuously Stirred Tank Reactor (CSTR) system.

\section{Background material}\label{sec:background-material}

\subsection{What is MPC ?}\label{sec:what-is-MPC}

We are given a discrete-time controlled nonlinear dynamical system of the form $x(k+1)=f(x(k), u(k))$ (which we will usually denote $x^+=f(x,u)$) with state and control constraints $x(k)\in\cal{X}\subseteq\R^{n_x},~u(k)\in\cal{U}\subseteq\R^{n_u}$.

In practice, the systems of interest are almost always continuous. We therefore assume that we will only change the control variables at instants evenly spaced in time, and we discretize the dynamics using ODE integrators such as RK4. The time between each two consecutive control updates is called the \textit{sampling time}.

Our goal is to stabilize the system around a target state $x^*$ and a target control $u^*$, which have to be a fixed point of the system : $x^*=f(x^*, u^*)$\,.
To do that we will try to construct a \textit{state-feedback control law}, i.e. a function $\mu:\cal{X}\to\cal{U}$ such that $x^*$ is \textit{asymptotically stable} for the dynamical system $x^+=f(x,\mu(x))$ (which only depends on the state).
The exact definition of asymptotica stability and how to prove it is discussed in subsection \ref{sec:Lyapunov-stability-theory}.

An important simplification that is often made (and that we will use in this paper) is that we consider $x^*=0$ and $u^*=0$\,.
It is easy to verify that when this is not the case, we can just define new translated states $\hat{x}=x-x^*$ and new control $\hat{u}=u-u^*$ and consequently new dynamics, new constraints, etc.

To construct this control law $\mu$, we will solve the following opitmization problem :
\begin{align}
	\begin{split}
		\label{eq:NMPC}
		V_N(x)=\underset{\bf{x},\bf{u}}{\min} &\quad \sum_{k=0}^{N-1}l(x_k,u_k)~+~F(x_N)\\
		\text{s.t.} &\quad x_0=x\text{ and }x_{k+1}=f(x_k,u_k),~k=0,\dots,N-1\\
		&\quad x_k\in\cal{X},~k=0,\dots,N\\
		&\quad u_k\in\cal{U},~k=0,\dots,N-1
	\end{split}
\end{align}
A bit of vocabulary : $N$ is called the \textit{horizon length}, the function $l$ is the \textit{stage cost}, the function $F$ is the \textit{terminal cost} (its role will become clearer in next section) and the function $J_N(\bf{x},\bf{u})=\sum_{k=0}^{N-1}l(x_k,u_k)~+~F(x_N)$ is called the \textit{cost function}.

These costs have to be defined byt the user as a measure of the deviations from the target state and control (we recall they are 0).
Their fine tuning is of capital importance for the feasibility, robustness and performance of the MPC scheme, and it is a whole area of interest of control theory (which is out of the scope of this paper).
Then, during the experiment, we observe at a time $k$ the state $x(k)$.
We solve the MPC \ref{eq:NMPC} and find the optimal sequences of states and controls $$\bf{x}^*(x(k))=\left\{ x_0^*(x(k))=x(k),\dots,x_N^*(x(k))\right\},\quad \bf{u}^*(x(k))=\left\{ u_0^*(x(k)),\dots,u_{N-1}^*(x(k))\right\}$$
They represent the controls to apply at time $k,k+1,\dots,k+N-1$ and the states the system will be in after these controls are applied.
In a sense, we 'over-solve' the problem since we are actually only interested in $u_0^*(x(k))$ that will define our control law :
$$\mu_{MPC}(x(k)):=u_0^*(x(k))$$
Since at time $k+1$ the horizon will have shifted, we say we solve the control problem in a \textit{receeding horizon} fashion.

\vspace{12pt}

This general form of the MPC is the one we will use, but other variants exist in which different state constraint, called \textit{terminal constraints}, are enforces on $x_N$\,.
We are also going to consider very specific form of costs and constraints : the costs will always be quadratic ($l(x,u)=x^TQx+u^TRu$ and $F(x)=x^TPx$ for some positive definite matrices $Q,R$ and $P$) and the (inequality) constraints will always be polytopic ($\cal{X}=\left\{x\in\R^{n_x}~|~C_xx\leq d_x\right\},~\cal{U}=\left\{u\in\R^{n_u}~|~C_uu\leq d_u\right\}$ for matrices $C_x,C_u$ and vectors $d_x,d_u$ of appropriate dimension).
These are very common in real-world applications.

\vspace{12pt}

Other variants of MPC are:
\begin{itemize}[label=\textbullet]
	\item \underline{the \textit{infinite horizon MPC}}:
	\begin{align*}
		V_\infty(x)=\underset{\bf{x},\bf{u}}{\min} &\quad \sum_{k=0}^{\infty}l(x_k,u_k)\\
		\text{s.t.} &\quad x_0=x\text{ and }x_{k+1}=f(x_k,u_k),~k=0,\dots,N-1\\
		&\quad x_k\in\cal{X},~k=0,\dots,N\\
		&\quad u_k\in\cal{U},~k=0,\dots,N-1
	\end{align*}
	In real life applications, we virtually consider that the duration of the experiment is infinite, so this variant may seem more natural.
	However this opitmization problem is infinite-dimensional so not easily solvable in practice.
	The terminal costs in regular MPC can actually be interpreted as a proxy for the part of the inifinite horizon that is not considered, and they are usually chosen this way.

	\item\underline{the \textit{(infinite-horizon) Linear Quadratic Regulator}, or \textit{LQR}}:\newline
	it is basically an MPC with linear dynamics ($x^+=Ax+Bu$) and without state and control constraints.
	These problems are the most basic one can find and are deeply linked to \textit{(Discrete) Algebraic Riccati Equations}, or \textit{DARE}:
	\begin{equation}
		\label{eq:DARE-LQR}
		P=Q+A^TPA-A^TPB(R+B^TPB)^{-1}B^TPA
	\end{equation}
	The unique positive definite solution of this equation actually verifies $V_\infty(x)=x^TPx$ so it can be used to define the terminal costs of the finite horizon LQR problem.
	This equation also provides the stabilizing state-feedback control law of the infinite horizon LQR : $\mu(x)=Kx$ where $K=-(R+B^TPB)^{-1}B^TPA$\,.
	It is intersting to notice that in this case, the solution $P$ to the DARE also verifies the follwing equation called \textit{Lyapunov equation} :
	$$P=A_K^TPA_K+Q_K$$
	where $A_K:=A+BK$ corresponds the evolution of the controlled system ($x^+=Ax+B\mu(x)=(A+BK)x$) and $Q_K:=Q+K^TRK$ corresponds to cost of a stage $(x,Kx)$ ($l(x,Kx)=x^TQ+(Kx)^TR(Kx)=x^TQ_Kx$).

	More details on the properties of the DARE can be found in the appendix \ref{sec:DARE}.
\end{itemize}


\subsection{Lyapunov stability theory}\label{sec:Lyapunov-stability-theory}
In this subsection we present the necessary background in dynamical systems theory and in particular give more precise definitions of the stability properties we want to prove.
We will consider a general nonlinear dynamical system $x^+=g(x)$, whithout any control, since we remember that our goal is to construct the control as only depending on the state.

A state $x^*$ is said to be :
\begin{itemize}[label=\textbullet]
	\item \textit{locally stable} if $\forall\epsilon>0,\exists \delta>0$ s.t. $\|x(0)-x^*\|\leq\delta\implies\|x(k)-x^*\|\leq\epsilon,~\forall k\geq 0$\,.
	\item \textit{locally attractive} if $\|x(k)-x^*\|\underset{k\to\infty}{\longrightarrow}0$ for all $x(0)$ in a certain neighborhood of $x^*$\,.
	\item \textit{globally attractive} if $\|x(k)-x^*\|\underset{k\to\infty}{\longrightarrow}0$ for all $x(0)$ in $\cal{X}$\,.
	\item \textit{locally asymptotically stable}, or \textit{LAS}, if it is both locally stable and locally attractive.
	\item \textit{globally asymptotically stable}, or \textit{GAS}, if it is both stable and globally attractive.
\end{itemize}

It can be showed that a state $x^*$ is GAS (or a LAS), if we can find a function $V:\cal{X}\to\R$ such that $\forall x\in\cal{X}$ (respectively in a neighborhood of $x^*$):
\begin{align*}
	V(x)&\leq c_1\|x-x^*\|^2\\
	V(x)&\geq c_2\|x-x^*\|^2\\
	V(g(x))&\leq V(x)-c_3\|x-x^*\|^2
\end{align*}
Such a function is called a \textit{Lyapunov function}, and the last equation is called the \textit{descent property}, which is usually the one that is the harder to prove for a given candidate function.

\vspace{12pt}

In our case, the candidate function we are going to be interested in is the objective value function $V_N$ defined in \ref{eq:NMPC}.


\section{The RRLB MPC}\label{sec:RRLB-MPC}

In this section we will introduce the RRLB functions, formulate the RRLB MPC and recall the results proved in the linear case in \cite{RRLB-linear-MPC}.
As mentioned in the last section, we will suppose that $x^*=0\in\cal{X}^\circ$ and $u^*=0\in\cal{U}^\circ$.

\vspace{12pt}

First let's take a step back and recall what regular log-barrier functions are.
We will first give the definitions only for the states $x$, since they only depend on the constraint set and not on the actual quantity represented.

For a simple constraint of the form $c^Tx\leq d$, the associated \textit{log-barrier function} is defined as $-\log(d-c^Tx)$.
Such a function is defined on the interior of feasible set of the constraint and becomes infinity near its boundary.
For a set of polytopic constraints, we can define the log-barrier as the sum of the log-barriers for each constraint :
$$B_x(x)=\sum_{i=1}^{q_x}-\log(d_{x,i}-\rm{row}_i(C_x)x)$$
where $q_x$ is the number of constraints, or equivalently the numbers of rows in $C_x$\,.

A first interesting property we would like to add to our log-barrier function is the one of \textit{positive definiteness}, i.e. $B_x(x)>0,~\forall x\neq 0$ in a neighborhood of the origin.
To this end we introduce the notion of \textit{weight recentered log-barrier function} as defined in \cite{RLB} :
$$B_x(x)=\sum_{i=1}^{q_x}(1+w_{x,i})\left[\log(d_{x,i})-\log(d_{x,i}-\rm{row}_i(C_x)x)\right]$$
where the weights $w_{x,i}$ are defined as chosen such that $B_x(0)=0$ and $\nabla B_x(0)=0$.
It is shown in \cite{RLB} that such weights always exist and that this wight-recentered log-barrier function is also positive definite and upper-bounded by a quadratic.

The second important property we have to add is the existence on the whole space.
For a given \textit{relaxation parameter} $\delta>0$, we can define the \textit{relaxed recentered log-barrier function}, or \textit{RRLB} function as follows :
\begin{align}
	\begin{split}
		\label{RRLB}
		B_x(x)&=\sum_{i=1}^{q_x}(1+w_{x,i})B_{x,i}(x)\\
		\text{ with }B_{x,i}(x)&=\begin{cases}
			\log(d_{x,i})-\log(d_{x,i}-\rm{row}_i(C_x)x)&\text{if }d_{x,i}-\rm{row}_i(C_x)x>\delta\\
			\beta(d_{x,i}-\rm{row}_i(C_x)x;\delta)&\text{ otherwise}
		\end{cases}
	\end{split}
\end{align}
where $\beta$ is a twice continuously function that extends the log-barrier to a twice continuously differentiable function on $\R$.
The simplest example of such a function is
$$\beta(z;\delta)=\frac{1}{2}\left[ \left( \frac{z-2\delta}{\delta} \right)^2-1 \right]-\log(\delta)$$
To ensure that this function still verifies $B_x(0)=0$ and $\nabla B_x(0)=0$, we need to choose $0<\delta\leq\min\left\{d_{x,1},\dots,d_{x,q_x},d_{u,1},\dots,d_{u,q_u}\right\}$\,, which is always possible if $0\in\cal{X}$ and $0\in\cal{U}$\,.

\vspace{24pt}

With all these definitions we can finally define the \textit{RRLB MPC} :
\begin{align}
	\begin{split}\label{eq:RRLB-NMPC}
		\tilde{V}_N(x)=\underset{\bf{x},\bf{u}}{\min} &\quad \tilde{J}_N(\bf{x},\bf{u})\\
		\text{s.t.} &\quad x_0=x\text{ and }x_{k+1}=f(x_k,u_k),~k=0,\dots,N-1\\
	\end{split}
\end{align}
where the new objective function $\tilde{J}_N(\bf{x},\bf{u})=\sum_{k=0}^{N-1}\tilde{l}(x_k,u_k)~+~\tilde{F}(x_N)$ is defined using the new stage costs $\tilde{l}(x,u)=l(x,u)+\epsilon B_x(x)+\epsilon B_u(u)$ and the new terminal cost $\tilde{F}(x)=x^TPx$ for another matrix $P$ that will be determined in such a way that the control law given by the MPC yields an asymptotically stable system.
The barrier parameter $\epsilon$ has in theory the following interpretation : when it goes to zero, the solution of problem \ref{eq:RRLB-NMPC} converges to the one of \ref{eq:NMPC}.
However, we will here consider it as fixed for simplicity.

\vspace{12pt}

When we consider the RRLB MPC \ref{eq:RRLB-NMPC} in the case of linear dynamics $x^+=Ax+Bu$, we know the two following theorems.

\begin{theorem}
	\label{nominal-stability-linear-case}
	Suppose that the pair $(A,B)$ is stabilizable\footnote{We did not discuss the notion of controllability and stabilizability in \ref{sec:what-is-MPC} because they do not play a direct role in our proofs. They are however very important in standard control theory and they are needed in the proofs of existence of DAREs. More on them in the appendix \ref{sec:controllability-stabilizability}}
	and that the matrix $P$ is chosen as the unique positive definite solution to the following modified DARE :
	$$P=A^TPA-A^TPB(R+\epsilon M_u+B^TPB)^{-1}B^TPA+Q+\epsilon M_x$$
	where $M_x=\nabla^2 B_x(0)$ and $M_u=\nabla^2 B_u(0)$\, are the hessian of the RRLB functions at the origin.

	Then for any initial state $x(0)\in\R^{n_x}$, the control law given by the MPC yields an asymptotically stable system.
	% TODO : add definition of stabilizable
\end{theorem}

\begin{theorem}
	\label{constraint-satisfaction-guarantee-linear-case}
	In the same conditions as previous theorem, there is a neighborhood $\cal{X}_N(\delta)$ of the origin such that for any initial state $x(0)\in\cal{X}_N(\delta)$, all the constraint will be satisfied along the closed-loop trajectories.
	Furthermore, the set $\cal{X}_N(\delta)$ is given explicitly by :
	$$\cal{X}_N(\delta)=\left\{x\in\cal{X}~|~\tilde{V}_N(x(0))-x(0)^TP_{LQR}x(0)\leq\min\{\beta_x(\delta),\beta_u(\delta)\}\right\}$$
	where $P_{LQR}$ is the solution to the classical DARE \ref{eq:DARE-LQR} and
	$$\beta_x(\delta)=\underset{i,x}{\min}\left\{ B_x(x)~|~\rm{row}_i(C_xx)=d_{x,i} \right\},\quad \beta_u(\delta)=\underset{i,u}{\min}\left\{ B_u(u)~|~\rm{row}_i(C_uu)=d_{u,i} \right\}$$
\end{theorem}

Additional results were proven in the linear case, such as the existence of a procedure to find $\delta$ such that the maximum constraint violation along the closed-loop trajectories is bounded by a pre-defined tolerance.
In our nonlinear case however we will not be able to prove these more powerful results because of the lack of knowledge on the error terms that appear in the dynamics (see subsection \ref{sec:constraints-satisfaction-guarantees})

\section{Theoretical properties of RRLB Nonlinear MPC}\label{sec:RRLB-theoretical-properties}

In this section, we consider the RRLB MPC \ref{eq:RRLB-NMPC} with nonlinear dynamics and show that the results of the previous section hold locally, in a certain neighborhood of the origin.

From this point on, for an initial state $x\in\R^{n_x}$ we will denote by \newline
$\tilde{\bf{x}}(x)=\left\{ \tilde{x}_0(x)=x,\tilde{x}_1(x),\dots,\tilde{x}_N(x) \right\}$ and $\tilde{\bf{u}}(x)=\left\{ \tilde{u}_0(x),\tilde{u}_1(x),\dots,\tilde{u}_{N-1}(x) \right\}$ the optimal sequences of states and controls found by the RRLB MPC.
When no confusion is possible we will drop the "$(x)$".

\subsection{Nominal asymptotic stability}\label{sec:RRLB-nominal-stability}

\begin{lemma}
	\label{thm:Lipchitzianity}
	Consider the RRLB MPC \ref{eq:RRLB-NMPC} and re-write it in a simpler way as
	$$\tilde{V}_N(x)=\underset{\bf{u}}{\min} \quad J(x,\bf{u})$$
	where $J(x,\bf{u})=\tilde{l}(x_0,u_0)+\tilde{l}(f(x_0,u_0),u_1)+\dots+\tilde{F}(f(f(...),u_{N-1}))$\,.
	If for a certain initial state $x$ we suppose that $D_\bf{u}J(x,\tilde{\bf{u}})=0$ and $\nabla_{\bf{u}\bf{u}}^2J(x, \tilde{\bf{u}})\succ 0$ (the matrix is positive definite) then in a neighborhood of $x$ we have:
	\begin{itemize}[label=\textbullet]
		\item $\forall k=0,\dots,N-1,\quad \|\tilde{u}_k(x)\|=O(\|x\|)$
		\item $\forall k=1,\dots,N,\quad \|\tilde{x}_k(x)\|=f(f(\dots,u_{k-2}),u_{k-1})=O(\|x\|)$
	\end{itemize}
\end{lemma}
\begin{proof}
	We can use the proof of the Theorem 4.2 in \cite{lectures-parametric-optimization} to argue that each $\tilde{x}_k(x)$ and $\tilde{u}_k$ is continuously differentiable on a neighborhood of the origin.
	Therefore, by continuity their gradient is bounded on this neighborhood and they are Lipchitz.
\end{proof}

Now our main result :

\begin{theorem}\label{thm:nominal-stability}
	Let's consider the problem \ref{eq:RRLB-NMPC} and assume the following :
	\begin{enumerate}
		\item If we denote the objective function as $J(x,\bf{u})$ as in Lemma \ref{thm:Lipchitzianity}, we have $D_\bf{u}J(0,\bf{u}(0))=D_\bf{u}J(0,0)=0$ and $\nabla^2_{\bf{u}\bf{u}}J(0,\bf{u}(0))=\nabla^2_{\bf{u}\bf{u}}J(0,0)\succ 0$

		\item When linearizing the system dynamics around the equilibrium and letting $A=D_xf(0,0),~B=D_uf(0,0)$, we suppose that the pair $(A,B)$ is stabilizable.
		This implies in particular that there exists a stabilizing cost $K$, i.e. a matrix such that $A_K:=A+BK$ only has eigenvalues in the unit disk.

		\item The matrix $P$ defining the terminal costs is the unique positive definite solution to the following Lyapunov equation :
		\begin{equation*}
			P=A_K^TPA_K+\mu Q_K
		\end{equation*}
		where $\mu>1$ and $Q_K=Q+\epsilon M_x+K^T(R+\epsilon M_u)K$\,.
	\end{enumerate}
	Then for all initial state in a certain neighborhood of the origin, the control law given by the RRLB MPC yields an asymptotically stable system, i.e. the origin is locally asymptotically stable for the dynamical system $x^+=f(x,\tilde{u}_0(x))$\,.
\end{theorem}

\begin{remark}~
	The matrix $K$ can be constructed using the DARE for the following modified infinite horizon LQR problem :
	\begin{align}
		\begin{split}\label{eq:inf-LQR}
			\underset{\bf{x},\bf{u}}{\min} &\quad \sum_{k=0}^\infty x_k^T(Q+\epsilon M_x)x_k+u_k^T(R+\epsilon M_u)u_k\\
			\text{s.t.} &\quad x_{k+1}=Ax_k+Bu_k,~k=0,\dots,N-1\\
		\end{split}
	\end{align}
	where $M_x=\nabla^2B_x(0)$ and $M_u=\nabla^2B_u(0)$, i.e.
	$$\begin{cases}
		K=-(R+B^T\Pi B)^{-1}B^T\Pi A&\\
		\Pi=A_K^T\Pi A_K+Q_K
	\end{cases}$$
	The only difference between this last equation and the Lyapunov equation in the statement of the theorem is the $\mu$ term.
	In theory it is very important, but in practice, we can actually take it very close to 1 and find $P$ and $K$ by solving
	$$\begin{cases}
		K=-(R+B^TP B)^{-1}B^TP A&\\
		P=A_K^TP A_K+Q_K
	\end{cases}$$
	More on that in \ref{sec:RRLB-numerical-experiments}.
\end{remark}

\begin{proof}~
	To prove the result we will show that $\tilde{V}_N$ is a Lyapunov function in a certain neighborhood of the origin.

	First, by writing the Taylor expansion of the stage costs $\tilde{l}$ we can see that
	\begin{align*}
		\tilde{l}(x,u)&=x^T[\nabla_{xx}^2\tilde{l}(0,0)] x+u^T[\nabla_{uu}^2\tilde{l}(0,0)]u+O(\|x\|^3+\|u\|^3)\\
		&=x^T[Q+\epsilon M_x]x+u^T[R+\epsilon M_u]u+O(\|x\|^3+\|u\|^3)
	\end{align*}
	so $\tilde{l}(x,Kx)=x^TQ_Kx+O(\|x\|^3)$\,.
	By the second assumption, we also have $\forall x\in\R^{n_x}$:
	$$\tilde{F}(A_Kx)+\mu x^T Q_K x-\tilde{F}(x)=0$$
	We can use the same reasoning as in the paragraph 2.5.5 of \cite[MPC Theory, Computation and Design]{MPC-book} to show that $\tilde{F}$ is a local Lyapunov funnction, i.e. there exists a neighborhood of the origin such that $\forall x$ in it :
	\begin{align*}
		\tilde{F}(f(x,Kx))+x^T Q_K x-\tilde{F}(x)&\leq 0\\
		\Longleftrightarrow\tilde{F}(f(x,Kx))-\tilde{F}(x)+\tilde{l}(x,Kx)&= O(\|x\|^3)
	\end{align*}

	What's more, by previous lemma, the predicted states are all Lipschitz with respect to the initial state, so we can choose an even smaller neighborhood such that $\tilde{x}_N(x)$ is also in it.

	Now using the optimal sequence of states and controls of the RRLB MPC starting at $x$, we can construct new feasible sequences for the problem starting at $\tilde{x}_1(x)$ as \newline
	$\bf{x}'=(\tilde{x}_1,\dots,\tilde{x}_N,f(\tilde{x}_N,K\tilde{x}_N))$ and $\bf{u}'=(\tilde{u}_1,\dots,\tilde{u}_{N-1},K\tilde{x}_N)$\,.
	Then we have :
	$$\tilde{V}_N(\tilde{x}_1)\leq\tilde{J}_N(\bf{x}',\bf{u}')=\underbrace{\tilde{J}_N(\tilde{\bf{x}},\tilde{\bf{u}})}_{=\tilde{V}_N(x)}~\underbrace{\underbrace{-\tilde{l}(x,\tilde{u}_0)}_{=O(\|x\|^2)}+\underbrace{\tilde{F}(f(\tilde{x}_N,K\tilde{x}_N))-\tilde{F}(\tilde{x}_N)+\tilde{l}(\tilde{x}_N,K\tilde{x}_N) }_{=O(\|\tilde{x}_N\|^3)=O(\|x\|^3)} }_{\leq-c\|x\|^2\text{ in a smaller nbh }}\checkmark$$
	We have therefore shown the descent property of our candidate Lyapunov function $\tilde{V}_N$\,.

	Finally, it is easy to show that $\tilde{V}_N$ is lower and upper bounded by coercive quadratic functions (see appendix), so that $\tilde{V}_N$ is indeed a Lyapunov function in a small neighborhood around the origin.
\end{proof}

\subsection{Constraints satisfaction guarantees}\label{sec:constraints-satisfaction-guarantees}
This section follows closely the section IV.D of \cite{RRLB-linear-MPC} but gives more loose results that cannot take us as far.
In particular, will show that we can ensure the existence of a neighborhood around the origin such that if we start in it, the states and controls along the closed-loop simulation will never violate the constraints.
However, this neighborhood cannot be easily computed because of the generality of the considered dynamics.

In the whole section, we the assumptions of Theorem \ref{thm:nominal-stability} to be true.

\begin{lemma}
	\label{thm:RRLB-bounds-guarantees}
	Consider the RRLB MPC \ref{eq:RRLB-NMPC} and an initial state $x(0)$ in the neighborhood given by Theorem \ref{thm:nominal-stability}.
	Let's denote by $\left\{x(0),x(1),\dots\right\}$ and $\left\{u(0), u(1),\dots\right\}$ the closed-loop state and control trajectories (given by $u(k)=\tilde{u}_0(x(k)),~x(k+1)=f(x(k),u(k))$).
	Then $\forall k\geq 0$:
	\begin{align*}
		B_x(x(k))&\leq\frac{1}{\epsilon}\left(\tilde{V}_N(x(0))-x(0)^TP_{LQR}x(0)-\sum_{k=0}^\infty\eta(x(k))\right)\\
		B_u(u(k))&\leq\frac{1}{\epsilon}\left(\tilde{V}_N(x(0))-x(0)^TP_{LQR}x(0)-\sum_{k=0}^\infty\eta(x(k))\right)
	\end{align*}
	where $P_{LQR}$ is the solution to the DARE associated \ref{eq:inf-LQR} and $\eta(x)=\tilde{l}(\tilde{x}_N(x),K\tilde{x}_N(x))+\tilde{F}(\tilde{x}_N(x))-\tilde{F}(f(\tilde{x}_N(x), K\tilde{x}_N(x)))$.
\end{lemma}

\begin{proof}
	In the proof of theorem \ref{thm:nominal-stability} we showed that $\forall k\geq 0$:
	\begin{multline*}
		\tilde{V}_N(x(k+1))-\tilde{V}_N(x(k))\leq-\tilde{l}(x(k),u(k))+\tilde{l}(\tilde{x}_N(x(k)), K\tilde{x}_N(x(k)))\\
		-\tilde{F}(\tilde{x}_N(x(k)))+\tilde{F}(f(\tilde{x}_N(x(k)),\tilde{x}_N(x(k))))
	\end{multline*}
	so by summing everything we get a telescopic sum that we can compute using the fact that the system is asymptotically stable so $\lim_{k\to\infty}\tilde{V}_N(x(k))=0$, we get :
	\begin{align*}
		\tilde{V}_N(x(0))&\geq\sum_{k=0}^\infty\tilde{l}(x(k),u(k))-\underbrace{\tilde{l}(\tilde{x}_N(x(k)), K\tilde{x}_N(x(k)))+\tilde{F}(\tilde{x}_N(x(k)))-\tilde{F}(f(\tilde{x}_N(x(k)),\tilde{x}_N(x(k))))}_{\eta(x(k))=O(\|x(0)\|^3)}\\
		&=\sum_{k=0}^\infty l(x(k), u(k))+\epsilon B_x(x(k))+\epsilon B_u(u(k))+\eta(x(k))\\
		&\geq x(0)^TP_{LQR}x(0)+\sum_{k=0}^\infty\eta(x(k))+\epsilon\sum_{k=0}^\infty B_x(x(k))+\epsilon\sum_{k=0}^\infty B_u(u(k))\\
	\end{align*}
	Note that even if we don't know a closed form for $\eta(x(k))$, we know that $\sum_{k=0}^\infty\eta(x(k))$ must be finite because it is bounded by $\tilde{V}_N(x(0))$\,.
	Now since the RRLB functions are all positive definite, we can easily conclude.
\end{proof}

We define for ease of notation the following quantities
\begin{align}
	\begin{split}
		\alpha(x(0))=\tilde{V}_N(x(0))-x(0)^TP_{LQR}x(0)-\sum_{k=0}^\infty\eta(x(k))
	\end{split}\\
	\begin{split}
		\beta_x=\underset{i,x}{\min}\{B_x(x)~|~\rm{row}_i(C_x)x=d_{x,i}\}
	\end{split}\\
	\begin{split}
		\beta_u=\underset{i,u}{\min}\{B_u(u)~|~\rm{row}_i(C_u)u=d_{u,i}\}
	\end{split}
\end{align}

Note that the bounds $\beta_x,\beta_u$ depend on the relaxation parameter $\delta$\,.

\begin{lemma}
	\label{thm:constraint-set-def-with-RRLB}
	For all relaxation parameter $\delta$:
	\begin{align*}
		\left\{x\in\R^{n_x}~|~B_x(x)\leq\beta_x\right\}\subseteq\cal{X}
		\left\{u\in\R^{n_u}~|~B_u(u)\leq\beta_u\right\}\subseteq\cal{U}
	\end{align*}
\end{lemma}

\begin{proof}
	See Lemma 1 in \cite{RRLB-linear-MPC}.
\end{proof}

\begin{theorem}
	In the same setting as lemma \ref{thm:RRLB-bounds-guarantees}, for any initial state $x(0)$ in the set
	$$\cal{X}_N(\delta):=\left\{x\in\R^{n_x}~|~\alpha(x(0))\leq\epsilon\min\left\{\beta_x,\beta_u\right\}\right\}$$
	there is no state or control constraint violation along the closed loop trajectories.
\end{theorem}

\begin{proof}
	For any $x(0)\in\cal{X}_N(\delta)$ it holds due to \ref{thm:RRLB-bounds-guarantees} that for all $k\geq 0,~\epsilon B_x(x(k))\leq\alpha(x(0))\leq\epsilon\beta_x$ so $B_x(x(k))\leq \beta_x$ and $x(k)\in\cal{X}$.
	The same reasoning applies on the controls.
\end{proof}

% \newpage

\section{Numerical aspects of RRLB}\label{sec:RRLB-numerical-aspects}

In this section we take a more concrete look at the numerical methods employed to solve MPC problems in practice.
One of the more important challenges of such implementations resides in the complexity of the NLP to be solved, that may not permit finding an accurate solution when the sampling time is very short.
This issue is deeply aggravated on embedded platforms where the computational power is often limited.
This is why, historically, the first successful implementations of MPC in industrial applications were found in chemical engineering, where the sampling time are usually long enough to find an accurate solutions.

A great deal of effort has been put in the past decaded to design algorithms that make best use of the computational power to deliver usable results on embedded systems that necessitate very fast response times, such as autonomous cars where the typical sampling time is in the order of the millisecond.
One that rapidly gained interest is a variant of \textit{Sequential Quadratic Programming} (or SQP) called \textit{Real Time Iteration} (or RTI).
Invented by Moritz Diehl and published in \cite{rti-diehl}, it can roughly be defined as SQP with a fixed number of iterations, usually only one, that makes use of the solutions found at the previous MPC predictions to warm-start the solver to ensure both computational efficiency and accuracy.

In subsection \ref{sec:RTI-scheme} we formally introduce the RTI scheme for general MPC and the specialize it for RRLB MPC, then briefly discuss it stability properties in subsection \ref{sec:RRLB-real-time-stability} and finally present numerical results verifying the theoretical properties of RRLB MPC and compare its performance with  regular MPC in subsection \ref{sec:RRLB-numerical-experiments}.

\subsection{The RTI scheme}\label{sec:RTI-scheme}

We assume in this section familiarity with the SQP framework for general NLPs, as described for example in \cite{nocedal-wright}, and describe it in the context of a general MPC with quadratic satge and terminal costs but with general dynamics and state/control constraints.
% TODO : talk about the assumptions of SQP ?
Just as in sections \ref{sec:RRLB-MPC} and \ref{sec:RRLB-theoretical-properties}, we are going to consider the target state and controls to be the origin, which will simplify our notations inspired from \cite{mpc-rti}.
With the following MPC :
\begin{align*}
	\mathrm{MPC}(x)=\underset{\bf{x},\bf{u}}{\min} &\quad \sum_{k=0}^{N-1}\begin{pmatrix}x_k\\u_k\end{pmatrix}^T\begin{pmatrix}
		Q & 0\\
		0 & R\\
	\end{pmatrix}\begin{pmatrix}x_k\\u_k\end{pmatrix}~+~x_N^TPx_N\\
	\text{s.t.} &\quad x_0=x\text{ and }x_{k+1}=f(x_k,u_k),~k=0,\dots,N-1\\
	&\quad h_x(x_k)\leq 0,~k=0,\dots,N\\
	&\quad h_u(u_k)\leq 0,~k=0,\dots,N-1
\end{align*}
we can define the following QP subproblem :
\begin{align}
	\begin{split}
		\label{eq:SQP-QP}
		\mathrm{QP}(x, \mathbf{x}_i, \mathbf{u}_i)=\underset{\Delta\mathbf{x}_i,\Delta\mathbf{u}_i}{\mathrm{argmin}} & \quad \sum_{k=0}^{N_1}\begin{pmatrix}\Delta x_{i,k}\\\Delta u_{i,k}\end{pmatrix}^T\begin{pmatrix}Q & 0\\0 & R\\\end{pmatrix}\begin{pmatrix}\Delta x_{i,k}\\\Delta u_{i,k}\end{pmatrix}+J_{i,k}^T\begin{pmatrix}\Delta x_{i,k}\\\Delta u_{i,k}\end{pmatrix}\\
		&\quad +\Delta x_{i,N}^TP\Delta x_{i,N}+J_{i,N}^T\begin{pmatrix}\Delta x_{i,N}\\\Delta u_{i,N}\end{pmatrix}\\
		\text{s.t.} &\quad \Delta x_0=x-x_{i,0}\\
		&\quad \Delta x_{i,k+1}=A_{i,k}\Delta x_{i,k}+B_{i,k}\Delta u_{i,k}+r_{i,k},~k=0,\dots,N-1\\
		&\quad C_{i,k}\Delta x_{i,k}+h_{x,i,k}\leq 0,~k=0,\dots,N\\
		&\quad D_{i,k}\Delta x_{i,k}+h_{u,i,k}\leq 0,~k=0,\dots,N-1
	\end{split}
\end{align}
with
\begin{align}
	\begin{split}
		\label{eq:SQP-sensitivities}
		&A_{i,k}=D_xf(x_{i,k}, u_{i,k}),~B_{i,k}=D_uf(x_{i,k}, u_{i,k}),~C_{i,k}=D_xh(x_{i,k}, u_{i,k}),~D_{i,k}=D_uh(x_{i,k}, u_{i,k})\\
		&r_{i,k}=f(x_{i,k}, u_{i,k})-x_{i,k+1},\quad h_{x,i,k}=h_x(x_{i,k}),\quad h_{u,i,k}(u_{i,k})
	\end{split}
\end{align}
and then the SQP method reads :
\begin{algorithm}[H]
	\caption{general SQP method for MPC}
	\label{algo:general-SQP}
	\begin{algorithmic}[1]
	\Procedure{SQP}{x, $\mathbf{x}^{init}$, $\mathbf{u}^{init}$}
	\State{$\mathbf{x}_0\gets\mathbf{x}^{init}$} \Comment{initial guess for the states specified by the user}
	\State{$\mathbf{u}_0\gets\mathbf{u}^{init}$} \Comment{initial guess for the controls specified by the user}
	\State{$k\gets 0$}
	\While{a certain stopping cirterion is not met}\\
		\qquad \quad Compute $r_{i,k}, h_{x,i,k}, h-{u,i,k}$ and the sensitivities $A_{i,k}, B_{i,k}, C_{i,k}, D_{i,k}$ using \ref{eq:SQP-sensitivities}
		\State{$(\Delta\mathbf{x}_i, \Delta\mathbf{u}_i)\gets \mathrm{QP}(x, \mathbf{x}_i, \mathbf{u}_i)$} \Comment{Find descent direction using \ref{eq:SQP-QP}}
		\State{$\mathbf{x}_{i+1}\gets\mathbf{x}_i+\Delta\mathbf{x}_i$}
		\State{$k\gets k+1$}
	\EndWhile
	\EndProcedure
	\end{algorithmic}
\end{algorithm}
We can note that in \cite{mpc-rti} the presented SQP procedure includes a line-search step that computes a step size $\alpha\in(0,1]$ such that enough progress is made at each iteration.
In the case of general SQP, this is particularly important to ensure convergence.
However, if the initial guesses $\mathbf{x}^{init},~\mathbf{u}^{init}$ are well chosen, we can take full steps at each iteration and therefore attain faster convergence.

The most natural way to construct this initial guess would be to \textit{shift} the solution found at the previous control update (or equivalently, at the previous call to the solver of algorithm \ref{algo:general-SQP}).
More precisely, if at time $k$ we found $\mathbf{x}=(x_0,\dots,x_N),\mathbf{u}=(u_0,\dots,u_{N-1})$ then our initial guess at time $k+1$ will be $\mathbf{x}^{init}=(x_1,\dots,x_N,f(x_N,u_{new})),\mathbf{u}^{init}=(u_1,\dots,u_{N-1},u_{new})$ where $u_new$ might be chosen using different strategies :
\begin{itemize}[label=\textbullet]
	\item using a \textit{locally stabilizing control law} \footnote{This could roughly be defined as a law that leaves the state inside the feasible set, and that does not move the state further from the reference state.} $\kappa:\cal{X}\to\cal{U}$ (whenever one is available) and setting $u_{new}=\kappa(x_N)$\,.
	An example of such locally stablizing control law can be the control law of the infinite horizon LQR mentionned in subsection \ref{sec:what-is-MPC}.

	\item using the reference control $u^*$ (which again we consider as $0$ for simplicity).

 	\item using the same control as in the previous stage : $u_{new}=u_{N-1}$\,.
\end{itemize}

This shifting heuristic is based on the fact that the dynamics $f$ should either exactly equal or at least close to the actual dynamics of the system so that this initial guess is very close to the actual solution.
When it is the case, and if the solution found at time $k$ is feasible, then so will be the initial guess for time $k+1$, and the very first iteration of algorithm \ref{algo:general-SQP} provides a very accurate estimate of the solution at time $k+1$.
Numerical illustrations of these statements can be found in \cite{mpc-rti}.
This method supposes that the very first prediction of a closed-loop experiment has to be computed exactly, but this can be done offline (i.e. before starting the simulation) with an arbitrary accuracy.

The RTI scheme relies on these remarks to find an accurate solution to the MPC problem and combines them with another important idea trying to solve the \textit{real-time dilemma} which can be summarized in the following way :

Upon obtaining a new state estimation $x$, a new SQP procedure can be started to find the solution of the MPC.
During the execution, the systems continues to evolve and by the time it finishes, the current state of the system won't be the one fed to the algorithm, making the prediction obsolete.

To overcome this issue, RTI separates the SQP procedure into a \textit{preparation phase} during which the computation of the sensitivites (i.e. all the terms in \ref{eq:SQP-sensitivities}) and the condensation (i.e. the creation of the sparse matrices defining the QP problem by concatenating the terms of each stage) of the problem are performed (which only depends on the previous call to the solver, and can therefore be started right after it, before the next state estimation is available), and a \textit{feedback phase} that solves the QP using the last available state estimation as initial state.

Instead of presenting RTI with pseudocode as in algorithm \ref{algo:general-SQP}, we will rather draw up a timeline resuming how the different steps of the algorithm are used in real situation :

\vskip 1cm

\begin{figure}[H]
	\label{fig:RTI-timeline}
	\startchronology[startyear=-3,stopyear=5,startdate=false,dates=false,color=black,width=\textwidth, arrow=true]
	\chronoperiode[color=cyan, dates=false]{-3}{0}{\textcolor{cyan}{off-line initialization}}
	\chronoevent[year=false]{0}{\color{green}{m}}
	\chronoperiode[color=red,datesstyle={t},stopdate=false]{0}{1}{\textcolor{red}{P}}
	\chronoevent[year=false]{1}{\color{green}{m}}
	\chronoperiode[color=blue,datesstyle={t},stopdate=false]{1}{2}{\textcolor{blue}{F}}
	\chronoevent[year=false]{2}{}
	\chronoperiode[color=red,datesstyle={t},stopdate=false]{2}{3}{\textcolor{red}{P}}
	\chronoevent[year=false]{3}{\color{green}{m}}
	\chronoperiode[color=blue,datesstyle={t},stopdate=false]{3}{4}{\textcolor{blue}{F}}
	\chronoevent[year=false]{4}{}
	\chronoperiode[color=red,datesstyle={t}]{4}{5}{\textcolor{red}{P}}
	\chronoevent[year=false]{5}{\color{green}{m}}
	\stopchronology
	\caption{RTI timeline}
\end{figure}

Several important points should be highlighted :
\begin{itemize}[label=\textbullet]
	\item Before the start of the experiment, we solve the problem at an arbitrary precision offline, when there is no limit on the running time of the solver.
	This supposes that we know in advance the initial state of the system, which is a common assumption.
	In this phase, we could use the general SQP with as many steps as necessary for full convergence (remember that we can't properly use RTI yet since we don't have a good initial guess), or any other algorithm to solver constrainted NLPs.

	\item The "\color{green}m\color{black}" labels indicate the state measurements of the system.
	In practice, we suppose they are done at a constant rate given by the sampling time $dt$, and we can thus say that $\rm{t}1-\rm{t}0=dt$ and $\rm{t}(2k+3)-\rm{t}(2k+1)=dt,~\forall k\geq 0$\,.

	\item The phases denoted by "\color{red}P\color{black}" and "\color{blue}F\color{black}" are the preparation and feedback phases of the RTI scheme as described above.

	\item During the experiment, the state evolves continuously, but the control paramenters are kept constant on all intervals of the form $[\rm{t}(2k),\rm{t}(2k+2))$ because we update the control only when a new one is computed.
\end{itemize}

More practical details on RTI will be given in subsection \ref{sec:RRLB-numerical-experiments} where we apply this scheme to the simulation of a real system.

\subsection{Asymptotic stability of RTI scheme}\label{sec:RRLB-real-time-stability}

Section \ref{sec:RRLB-theoretical-properties} proved several important theoretical properties of RRLB MPC, and the next subsection presents some experimental verifications of these theorems.
However, it should be noted that all the solution to problem \ref{eq:RRLB-NMPC} found in these experiments are by essence inexact, and we can therefore not easily claim that the asymptotic stability and the constraint satisfaction still hold in practice.
Moritz Diehl, the inventor of RTI, studied under which assumptions such properties could be at least partially preserved.
We discuss them briefly but without any rigorous mathematical proof that would without a doubt be out of scope for this project.

\subsection{Numerical experiments}\label{sec:RRLB-numerical-experiments}

CSTR described in \cite{diehl-dissertation} and \cite{cstr-ref}

\newpage

\section{Appendix}
\printbibliography

\subsection*{Controllable vs. Stabilizable}\label{sec:controllability-stabilizability}

\subsection*{More on the DARE}\label{sec:DARE}

\begin{lemma}[Existence an Unicity of solution to the Lyapunov equation]

\end{lemma}

\begin{proof}

\end{proof}

\subsection*{Misc}

\begin{proof}[Proof of lemma \ref{thm:Lipchitzianity}]

\end{proof}

\end{document}
